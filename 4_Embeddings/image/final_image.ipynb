{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation des datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_text_test_embeddings = pd.read_csv('final_text_test_embeddings_flattened.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_identifiers = df_text_test_embeddings[['imageid', 'productid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 84916 entries, 0 to 84915\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   designation  84916 non-null  object\n",
      " 1   description  55116 non-null  object\n",
      " 2   productid    84916 non-null  int64 \n",
      " 3   imageid      84916 non-null  int64 \n",
      " 4   prdtypecode  84916 non-null  int64 \n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 5.9+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Chargement des fichiers \"X_train_uptade.csv\" et \"Y_trainCVw08PX.csv\"\n",
    "df_1 = pd.read_csv('X_train.csv', index_col=0)\n",
    "df_2 = pd.read_csv('y_train.csv', index_col=0)\n",
    "\n",
    "# Fusion avec merge des deux datasets\n",
    "df_classes = pd.merge(df_1, df_2, left_index = True, right_index = True)\n",
    "\n",
    "df_classes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classes = df_classes[['imageid', 'productid', 'prdtypecode']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création de Nom Image et lien\n",
    "df_classes['Nom image'] = ['image_' + str(imageid) + '_product_' + str(productid) + '.jpg' for imageid, productid in zip(df_classes['imageid'], df_classes['productid'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './datasets/images_train_upscalled'\n",
    "df_classes['lien'] = str(path) + '/' + df_classes['prdtypecode'].astype(str)+ '/' + df_classes['Nom image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusionner df_classes avec df_test_identifiers en utilisant la colonne 'imageid' pour récupérer 'prdtypecode'\n",
    "df_classes_test = df_classes.merge(df_test_identifiers['imageid'], on='imageid', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16984 entries, 0 to 16983\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   imageid      16984 non-null  int64 \n",
      " 1   productid    16984 non-null  int64 \n",
      " 2   prdtypecode  16984 non-null  int64 \n",
      " 3   Nom image    16984 non-null  object\n",
      " 4   lien         16984 non-null  object\n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 663.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_classes_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout du mapping des classes pour les images\n",
    "classe_images = pd.read_csv('class_images_mapping.csv')\n",
    "classe_images.rename(columns={'Class Name': 'prdtypecode', 'label': 'class_image'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prdtypecode</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1140</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1160</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1180</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1280</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1281</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1300</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1301</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1302</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1320</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1560</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1920</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1940</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2060</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2220</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2280</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2403</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2462</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2522</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2582</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2583</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2585</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2705</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2905</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>40</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>60</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    prdtypecode  Label\n",
       "0            10      0\n",
       "1          1140      1\n",
       "2          1160      2\n",
       "3          1180      3\n",
       "4          1280      4\n",
       "5          1281      5\n",
       "6          1300      6\n",
       "7          1301      7\n",
       "8          1302      8\n",
       "9          1320      9\n",
       "10         1560     10\n",
       "11         1920     11\n",
       "12         1940     12\n",
       "13         2060     13\n",
       "14         2220     14\n",
       "15         2280     15\n",
       "16         2403     16\n",
       "17         2462     17\n",
       "18         2522     18\n",
       "19         2582     19\n",
       "20         2583     20\n",
       "21         2585     21\n",
       "22         2705     22\n",
       "23         2905     23\n",
       "24           40     24\n",
       "25           50     25\n",
       "26           60     26"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classe_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation des images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# On ajuste le lien au nouveau lien pour les données de test\n",
    "df_classes_test['lien_test'] = df_classes_test['lien'].str.replace('images_train_upscalled', 'images_test_upscalled')\n",
    "\n",
    "# On créer les dossiers et fichiers pour le test !!!! (si déjà fait pas besoin de re-run ce code)\n",
    "def copy_images(row):\n",
    "    os.makedirs(os.path.dirname(row['lien_test']), exist_ok=True)\n",
    "    shutil.copy(row['lien'], row['lien_test'])\n",
    "\n",
    "# on applique la fonction pour chaque ligne du dataframe\n",
    "df_classes_test.apply(copy_images, axis=1)\n",
    "df_classes_test.to_csv(\"./final_image_test_dataset.csv\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement du model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import ViTForImageClassification, ViTConfig\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# load du model\n",
    "def load_model(filepath, device='cpu'):\n",
    "    device = torch.device(device)\n",
    "    config = ViTConfig.from_pretrained('google/vit-base-patch16-224-in21k', num_labels=27, output_hidden_states=True)\n",
    "    model = ViTForImageClassification(config)\n",
    "    try:\n",
    "        checkpoint = torch.load(filepath, map_location=device)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "    except KeyError:\n",
    "        model.load_state_dict(torch.load(filepath, map_location=device))\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "model = load_model(\"final_model_image.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"C:/Users/tgp/Documents/kaggle/Rakuten/Github_final/4_Prediction_catégories/datasets/images_test_upscalled\"\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "test_dataset = datasets.ImageFolder(dataset_path, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16984 entries, 0 to 16983\n",
      "Columns: 770 entries, 0 to path\n",
      "dtypes: float32(768), int64(1), object(1)\n",
      "memory usage: 50.0+ MB\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings_and_predictions(model, dataloader):\n",
    "    model.eval()\n",
    "    embeddings, predictions, paths = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            embeddings.extend(outputs.hidden_states[-1][:, 0, :].cpu().numpy())  # Extract the CLS token embeddings from last hidden state\n",
    "            _, preds = torch.max(outputs.logits, 1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            start_index = batch_idx * dataloader.batch_size\n",
    "            # récupération des chemins de chaque image (pour les utiliser comme identifiers)\n",
    "            batch_paths = [dataloader.dataset.samples[i][0] for i in range(start_index, start_index + len(labels))]\n",
    "            paths.extend(batch_paths)\n",
    "    return embeddings, predictions, paths\n",
    "\n",
    "embeddings, predictions, paths = get_embeddings_and_predictions(model, test_loader)\n",
    "\n",
    "embeddings_df = pd.DataFrame(embeddings)\n",
    "embeddings_df['predictions'] = predictions\n",
    "embeddings_df['path'] = paths\n",
    "\n",
    "embeddings_df.info()\n",
    "embeddings_df.to_csv(\"final_image_test_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([            0,             1,             2,             3,\n",
       "                   4,             5,             6,             7,\n",
       "                   8,             9,\n",
       "       ...\n",
       "                 760,           761,           762,           763,\n",
       "                 764,           765,           766,           767,\n",
       "       'predictions',        'path'],\n",
       "      dtype='object', length=770)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
       "       ...\n",
       "       '44992', '44993', '44994', '44995', '44996', '44997', '44998', '44999',\n",
       "       'imageid', 'productid'],\n",
       "      dtype='object', length=45001)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_test_embeddings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df['Nom image'] = embeddings_df['path'].str.extract(r'.*\\\\(image_.*)')\n",
    "\n",
    "# Split the 'Nom image' column to extract 'imageid' and 'productid'\n",
    "split_columns = embeddings_df['Nom image'].str.split('_', expand=True)\n",
    "\n",
    "# Create 'imageid' and 'productid' columns in embeddings_df\n",
    "embeddings_df['imageid'] = split_columns[1]\n",
    "embeddings_df['productid'] = split_columns[3].str.replace('.jpg', '')\n",
    "\n",
    "embeddings_df.drop(columns=['path'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([            0,             1,             2,             3,\n",
       "                   4,             5,             6,             7,\n",
       "                   8,             9,\n",
       "       ...\n",
       "                 762,           763,           764,           765,\n",
       "                 766,           767, 'predictions',   'Nom image',\n",
       "           'imageid',   'productid'],\n",
       "      dtype='object', length=772)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_predictions = embeddings_df[['imageid', 'productid', 'Nom image', 'predictions']]\n",
    "test_image_predictions.to_csv(\"./final_image_test_predictions.csv\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
       "       ...\n",
       "       '44992', '44993', '44994', '44995', '44996', '44997', '44998', '44999',\n",
       "       'imageid', 'productid'],\n",
       "      dtype='object', length=45001)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_test_embeddings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_test_embeddings = embeddings_df.drop(columns=['predictions', 'Nom image'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([          0,           1,           2,           3,           4,\n",
       "                 5,           6,           7,           8,           9,\n",
       "       ...\n",
       "               760,         761,         762,         763,         764,\n",
       "               765,         766,         767,   'imageid', 'productid'],\n",
       "      dtype='object', length=770)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_image_test_embeddings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_test_embeddings.to_csv(\"./final_image_test_embeddings_clean.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut préparer le même dataset d'embeddings pour les données d'entrainement. Mais pour celà il faut déjà préparer les images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 84916 entries, 0 to 84915\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   imageid      84916 non-null  int64 \n",
      " 1   productid    84916 non-null  int64 \n",
      " 2   prdtypecode  84916 non-null  int64 \n",
      " 3   Nom image    84916 non-null  object\n",
      " 4   lien         84916 non-null  object\n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 5.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_classes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16984 entries, 0 to 16983\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   imageid      16984 non-null  int64 \n",
      " 1   productid    16984 non-null  int64 \n",
      " 2   prdtypecode  16984 non-null  int64 \n",
      " 3   Nom image    16984 non-null  object\n",
      " 4   lien         16984 non-null  object\n",
      " 5   lien_test    16984 non-null  object\n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 796.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_classes_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a left merge on df_classes with df_classes_test using 'imageid' to find common entries\n",
    "merged_df = df_classes.merge(df_classes_test[['imageid']], on='imageid', how='left', indicator=True)\n",
    "\n",
    "# Filter rows where 'imageid' does not exist in df_classes_test\n",
    "result_df = merged_df[merged_df['_merge'] == 'left_only']\n",
    "\n",
    "# Drop the indicator column as it's no longer needed\n",
    "result_df = result_df.drop(columns=['_merge'])\n",
    "\n",
    "# Create the new DataFrame with rows from df_classes excluding those found in df_classes_test\n",
    "df_exclusive_classes = result_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 67932 entries, 0 to 84914\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   imageid      67932 non-null  int64 \n",
      " 1   productid    67932 non-null  int64 \n",
      " 2   prdtypecode  67932 non-null  int64 \n",
      " 3   Nom image    67932 non-null  object\n",
      " 4   lien         67932 non-null  object\n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_exclusive_classes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# On ajuste le lien au nouveau lien pour les données de test\n",
    "df_exclusive_classes['lien_train_final'] = df_exclusive_classes['lien'].str.replace('images_train_upscalled', 'images_train_upscalled_final')\n",
    "\n",
    "# On créer les dossiers et fichiers pour le test !!!! (si déjà fait pas besoin de re-run ce code)\n",
    "def copy_images(row):\n",
    "    os.makedirs(os.path.dirname(row['lien_train_final']), exist_ok=True)\n",
    "    shutil.copy(row['lien'], row['lien_train_final'])\n",
    "\n",
    "# on applique la fonction pour chaque ligne du dataframe\n",
    "df_exclusive_classes.apply(copy_images, axis=1)\n",
    "df_exclusive_classes.to_csv(\"./final_image_train_dataset.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 67932 entries, 0 to 84914\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   imageid           67932 non-null  int64 \n",
      " 1   productid         67932 non-null  int64 \n",
      " 2   prdtypecode       67932 non-null  int64 \n",
      " 3   Nom image         67932 non-null  object\n",
      " 4   lien              67932 non-null  object\n",
      " 5   lien_train_final  67932 non-null  object\n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_exclusive_classes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"C:/Users/tgp/Documents/kaggle/Rakuten/Github_final/4_Prediction_catégories/datasets/images_train_upscalled_final\"\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "test_dataset = datasets.ImageFolder(dataset_path, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0     imageid   productid  prdtypecode  \\\n",
      "0           0  1263597046  3804725264           10   \n",
      "1           1  1008141237   436067568         2280   \n",
      "2           2   938777978   201115110           50   \n",
      "3           3   457047496    50418756         1280   \n",
      "4           4  1077757786   278535884         2705   \n",
      "\n",
      "                                 Nom image  \\\n",
      "0  image_1263597046_product_3804725264.jpg   \n",
      "1   image_1008141237_product_436067568.jpg   \n",
      "2    image_938777978_product_201115110.jpg   \n",
      "3     image_457047496_product_50418756.jpg   \n",
      "4   image_1077757786_product_278535884.jpg   \n",
      "\n",
      "                                                lien  \\\n",
      "0  ./datasets/images_train_upscalled/10/image_126...   \n",
      "1  ./datasets/images_train_upscalled/2280/image_1...   \n",
      "2  ./datasets/images_train_upscalled/50/image_938...   \n",
      "3  ./datasets/images_train_upscalled/1280/image_4...   \n",
      "4  ./datasets/images_train_upscalled/2705/image_1...   \n",
      "\n",
      "                                    lien_train_final  \n",
      "0  ./datasets/images_train_upscalled_final/10/ima...  \n",
      "1  ./datasets/images_train_upscalled_final/2280/i...  \n",
      "2  ./datasets/images_train_upscalled_final/50/ima...  \n",
      "3  ./datasets/images_train_upscalled_final/1280/i...  \n",
      "4  ./datasets/images_train_upscalled_final/2705/i...  \n"
     ]
    }
   ],
   "source": [
    "# Si on reprend le code ici et on a besoin de charger df_exclusive_classes !!!! \n",
    "import pandas as pd\n",
    "df_exclusive_classes = pd.read_csv(\"./final_image_train_dataset.csv\")\n",
    "print(df_exclusive_classes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_embeddings_and_predictions optimizé pour un dataset de grande taille comme ici\n",
    "import torch\n",
    "\n",
    "def get_embeddings_and_predictions_generator(model, dataloader):\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    with torch.no_grad():  # Context-manager that disables gradient calculation\n",
    "        for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "            images = images.to(device)  # Move images to the appropriate device (e.g., GPU)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Extract embeddings (e.g., CLS token from last hidden state)\n",
    "            embeddings = outputs.hidden_states[-1][:, 0, :].cpu().numpy()\n",
    "            _, preds = torch.max(outputs.logits, 1)  # Get predictions\n",
    "            predictions = preds.cpu().numpy()\n",
    "            \n",
    "            # Retrieve paths for batch items\n",
    "            start_index = batch_idx * dataloader.batch_size\n",
    "            batch_paths = [dataloader.dataset.samples[i][0] for i in range(start_index, start_index + len(labels))]\n",
    "            \n",
    "            yield embeddings, predictions, batch_paths  # Yield the results for this batch\n",
    "\n",
    "# Example usage of the generator\n",
    "for embeddings, predictions, paths in get_embeddings_and_predictions_generator(model, test_loader):\n",
    "    # Here, process each batch immediately, for example, append to a CSV or database incrementally\n",
    "    embeddings_df_batch = pd.DataFrame(embeddings)\n",
    "    embeddings_df_batch['predictions'] = predictions\n",
    "    embeddings_df_batch['path'] = paths\n",
    "    \n",
    "    # Write to CSV incrementally, appending each batch\n",
    "    embeddings_df_batch.to_csv(\"final_image_train_embeddings.csv\", mode='a', header=not os.path.exists(\"final_image_train_embeddings.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "      <th>predictions</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.698478</td>\n",
       "      <td>7.941746</td>\n",
       "      <td>-0.050884</td>\n",
       "      <td>5.949044</td>\n",
       "      <td>-3.486584</td>\n",
       "      <td>-0.252941</td>\n",
       "      <td>8.432141</td>\n",
       "      <td>-2.330757</td>\n",
       "      <td>-6.713544</td>\n",
       "      <td>-4.273891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.846183</td>\n",
       "      <td>-5.175003</td>\n",
       "      <td>3.241985</td>\n",
       "      <td>0.895443</td>\n",
       "      <td>-6.327821</td>\n",
       "      <td>8.990051</td>\n",
       "      <td>2.118797</td>\n",
       "      <td>5.822665</td>\n",
       "      <td>26</td>\n",
       "      <td>C:/Users/tgp/Documents/kaggle/Rakuten/Github_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.799744</td>\n",
       "      <td>2.801049</td>\n",
       "      <td>3.154672</td>\n",
       "      <td>3.397383</td>\n",
       "      <td>-1.684122</td>\n",
       "      <td>-6.680769</td>\n",
       "      <td>-0.294301</td>\n",
       "      <td>-4.656973</td>\n",
       "      <td>-9.092684</td>\n",
       "      <td>5.499386</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102222</td>\n",
       "      <td>-0.308481</td>\n",
       "      <td>-2.926835</td>\n",
       "      <td>-1.648580</td>\n",
       "      <td>-8.292006</td>\n",
       "      <td>5.910584</td>\n",
       "      <td>2.053848</td>\n",
       "      <td>1.961516</td>\n",
       "      <td>18</td>\n",
       "      <td>C:/Users/tgp/Documents/kaggle/Rakuten/Github_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.360548</td>\n",
       "      <td>6.594135</td>\n",
       "      <td>4.147007</td>\n",
       "      <td>2.649189</td>\n",
       "      <td>1.359341</td>\n",
       "      <td>-1.925660</td>\n",
       "      <td>-6.416381</td>\n",
       "      <td>-5.164004</td>\n",
       "      <td>-0.915812</td>\n",
       "      <td>-1.593801</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.726437</td>\n",
       "      <td>-3.467264</td>\n",
       "      <td>-2.390601</td>\n",
       "      <td>4.002279</td>\n",
       "      <td>-3.006968</td>\n",
       "      <td>2.197342</td>\n",
       "      <td>-0.216316</td>\n",
       "      <td>0.249161</td>\n",
       "      <td>25</td>\n",
       "      <td>C:/Users/tgp/Documents/kaggle/Rakuten/Github_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.493410</td>\n",
       "      <td>8.795631</td>\n",
       "      <td>-1.184605</td>\n",
       "      <td>3.017137</td>\n",
       "      <td>-4.657485</td>\n",
       "      <td>-2.384127</td>\n",
       "      <td>6.009043</td>\n",
       "      <td>-1.682365</td>\n",
       "      <td>-8.383932</td>\n",
       "      <td>0.272733</td>\n",
       "      <td>...</td>\n",
       "      <td>1.396728</td>\n",
       "      <td>0.134982</td>\n",
       "      <td>4.427821</td>\n",
       "      <td>-3.566312</td>\n",
       "      <td>-8.955771</td>\n",
       "      <td>6.066698</td>\n",
       "      <td>-0.412958</td>\n",
       "      <td>-7.171250</td>\n",
       "      <td>5</td>\n",
       "      <td>C:/Users/tgp/Documents/kaggle/Rakuten/Github_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.071945</td>\n",
       "      <td>3.124883</td>\n",
       "      <td>2.728526</td>\n",
       "      <td>5.936134</td>\n",
       "      <td>-4.932726</td>\n",
       "      <td>2.276963</td>\n",
       "      <td>-8.110972</td>\n",
       "      <td>-8.461752</td>\n",
       "      <td>-1.379716</td>\n",
       "      <td>0.042778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292358</td>\n",
       "      <td>-2.734223</td>\n",
       "      <td>2.346735</td>\n",
       "      <td>-0.689087</td>\n",
       "      <td>-3.807111</td>\n",
       "      <td>-2.812330</td>\n",
       "      <td>6.517946</td>\n",
       "      <td>-2.787428</td>\n",
       "      <td>25</td>\n",
       "      <td>C:/Users/tgp/Documents/kaggle/Rakuten/Github_f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 770 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  1.698478  7.941746 -0.050884  5.949044 -3.486584 -0.252941  8.432141   \n",
       "1  6.799744  2.801049  3.154672  3.397383 -1.684122 -6.680769 -0.294301   \n",
       "2 -1.360548  6.594135  4.147007  2.649189  1.359341 -1.925660 -6.416381   \n",
       "3  6.493410  8.795631 -1.184605  3.017137 -4.657485 -2.384127  6.009043   \n",
       "4  1.071945  3.124883  2.728526  5.936134 -4.932726  2.276963 -8.110972   \n",
       "\n",
       "          7         8         9  ...       760       761       762       763  \\\n",
       "0 -2.330757 -6.713544 -4.273891  ...  0.846183 -5.175003  3.241985  0.895443   \n",
       "1 -4.656973 -9.092684  5.499386  ... -0.102222 -0.308481 -2.926835 -1.648580   \n",
       "2 -5.164004 -0.915812 -1.593801  ... -0.726437 -3.467264 -2.390601  4.002279   \n",
       "3 -1.682365 -8.383932  0.272733  ...  1.396728  0.134982  4.427821 -3.566312   \n",
       "4 -8.461752 -1.379716  0.042778  ...  0.292358 -2.734223  2.346735 -0.689087   \n",
       "\n",
       "        764       765       766       767  predictions  \\\n",
       "0 -6.327821  8.990051  2.118797  5.822665           26   \n",
       "1 -8.292006  5.910584  2.053848  1.961516           18   \n",
       "2 -3.006968  2.197342 -0.216316  0.249161           25   \n",
       "3 -8.955771  6.066698 -0.412958 -7.171250            5   \n",
       "4 -3.807111 -2.812330  6.517946 -2.787428           25   \n",
       "\n",
       "                                                path  \n",
       "0  C:/Users/tgp/Documents/kaggle/Rakuten/Github_f...  \n",
       "1  C:/Users/tgp/Documents/kaggle/Rakuten/Github_f...  \n",
       "2  C:/Users/tgp/Documents/kaggle/Rakuten/Github_f...  \n",
       "3  C:/Users/tgp/Documents/kaggle/Rakuten/Github_f...  \n",
       "4  C:/Users/tgp/Documents/kaggle/Rakuten/Github_f...  \n",
       "\n",
       "[5 rows x 770 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_df_batch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([            0,             1,             2,             3,\n",
       "                   4,             5,             6,             7,\n",
       "                   8,             9,\n",
       "       ...\n",
       "                 762,           763,           764,           765,\n",
       "                 766,           767, 'predictions',   'Nom image',\n",
       "           'imageid',   'productid'],\n",
       "      dtype='object', length=772)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_df_batch['Nom image'] = embeddings_df_batch['path'].str.extract(r'.*\\\\(image_.*)')\n",
    "\n",
    "# Split the 'Nom image' column to extract 'imageid' and 'productid'\n",
    "split_columns = embeddings_df_batch['Nom image'].str.split('_', expand=True)\n",
    "\n",
    "# Create 'imageid' and 'productid' columns in embeddings_df_batch\n",
    "embeddings_df_batch['imageid'] = split_columns[1]\n",
    "embeddings_df_batch['productid'] = split_columns[3].str.replace('.jpg', '')\n",
    "\n",
    "embeddings_df_batch.drop(columns=['path'], inplace=True)\n",
    "\n",
    "df_image_train_embeddings = embeddings_df_batch\n",
    "\n",
    "df_image_train_embeddings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([          0,           1,           2,           3,           4,\n",
       "                 5,           6,           7,           8,           9,\n",
       "       ...\n",
       "               760,         761,         762,         763,         764,\n",
       "               765,         766,         767,   'imageid', 'productid'],\n",
       "      dtype='object', length=770)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_image_train_embeddings.drop(columns=['predictions', 'Nom image'], inplace=True)\n",
    "df_image_train_embeddings.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement du model final sur les embeddings textes + images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask_expr.DataFrame'>\n",
      "Columns: 45002 entries, 0 to productid\n",
      "dtypes: float64(45002)"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Charger le fichier CSV en utilisant Dask. Dask lit le fichier en partitions automatiquement.\n",
    "df_text_test_embeddings = dd.read_csv('./final_text_test_embeddings_flattened.csv', assume_missing=True, sample=10000000)\n",
    "df_text_test_embeddings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask_expr.DataFrame'>\n",
      "Columns: 770 entries, 0 to productid\n",
      "dtypes: float64(770)"
     ]
    }
   ],
   "source": [
    "df_image_test_embeddings = dd.read_csv('./final_image_test_embeddings_clean.csv', assume_missing=True)\n",
    "df_image_test_embeddings = df_image_test_embeddings.drop(columns=['Unnamed: 0'])\n",
    "df_image_test_embeddings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask_expr.DataFrame'>\n",
      "Columns: 45002 entries, 0 to productid\n",
      "dtypes: float64(45002)"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Charger le fichier CSV en utilisant Dask. Dask lit le fichier en partitions automatiquement.\n",
    "df_text_train_embeddings = dd.read_csv('./final_text_train_embeddings_flattened.csv', assume_missing=True, sample=10000000)\n",
    "df_text_train_embeddings.info()\n",
    "\n",
    "# Si vl'on a besoin d'exécuter des opérations qui requièrent le DataFrame complet en mémoire (à éviter si très volumineux),\n",
    "# on peut appeler .compute() pour obtenir un DataFrame Pandas.\n",
    "# df_text_train_embeddings_computed = df_text_train_embeddings.compute()  # Utiliser avec précaution !\n",
    "\n",
    "# Si l'on veut voir la structure du calcul (graphique des tâches) sans exécuter le calcul :\n",
    "# df_text_train_embeddings.visualize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_image_train_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_image_train_embeddings \u001b[38;5;241m=\u001b[39m dd\u001b[38;5;241m.\u001b[39mfrom_pandas(\u001b[43mdf_image_train_embeddings\u001b[49m, npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      2\u001b[0m df_image_train_embeddings\u001b[38;5;241m.\u001b[39minfo()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_image_train_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "df_image_train_embeddings = dd.from_pandas(df_image_train_embeddings, npartitions=10)\n",
    "df_image_train_embeddings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask_expr.DataFrame'>\n",
      "Columns: 771 entries, Unnamed: 0 to path\n",
      "dtypes: float64(770), string(1)"
     ]
    }
   ],
   "source": [
    "df_image_train_embeddings = dd.read_csv('./final_image_train_embeddings.csv', assume_missing=True, sample=10000000)\n",
    "df_image_train_embeddings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Column assignment doesn't support type <class 'dask_expr._collection.DataFrame'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_image_train_embeddings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNom image\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_image_train_embeddings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mextract(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.*\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m(image_.*)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Split the 'Nom image' column to extract 'imageid' and 'productid'\u001b[39;00m\n\u001b[0;32m      4\u001b[0m split_columns \u001b[38;5;241m=\u001b[39m df_image_train_embeddings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNom image\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m, expand\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask_expr\\_collection.py:3005\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3003\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mItem assignment with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(key)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3004\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3005\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massign(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{key: value})\n\u001b[0;32m   3006\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expr \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39m_expr\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask_expr\\_collection.py:2783\u001b[0m, in \u001b[0;36mDataFrame.assign\u001b[1;34m(self, **pairs)\u001b[0m\n\u001b[0;32m   2779\u001b[0m         v \u001b[38;5;241m=\u001b[39m from_dask_array(\n\u001b[0;32m   2780\u001b[0m             v, index\u001b[38;5;241m=\u001b[39mresult\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mto_legacy_dataframe(), meta\u001b[38;5;241m=\u001b[39mresult\u001b[38;5;241m.\u001b[39m_meta\n\u001b[0;32m   2781\u001b[0m         )\n\u001b[0;32m   2782\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2783\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn assignment doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt support type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(v)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2784\u001b[0m     args\u001b[38;5;241m.\u001b[39mextend([k, v])\n\u001b[0;32m   2786\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: Column assignment doesn't support type <class 'dask_expr._collection.DataFrame'>"
     ]
    }
   ],
   "source": [
    "df_image_train_embeddings['Nom image'] = df_image_train_embeddings['path'].str.extract(r'.*\\\\(image_.*)')\n",
    "\n",
    "# Split the 'Nom image' column to extract 'imageid' and 'productid'\n",
    "split_columns = df_image_train_embeddings['Nom image'].str.split('_', expand=True)\n",
    "\n",
    "# Create 'imageid' and 'productid' columns in embeddings_df_batch\n",
    "df_image_train_embeddings['imageid'] = split_columns[1]\n",
    "df_image_train_embeddings['productid'] = split_columns[3].str.replace('.jpg', '')\n",
    "\n",
    "df_image_train_embeddings.drop(columns=['predictions', 'Nom image'], inplace=True)\n",
    "\n",
    "df_image_train_embeddings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "To use the expand parameter you must specify the number of expected splits with the n= parameter. Usually n splits result in n+1 output columns.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m df_image_train_embeddings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNom image\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_image_train_embeddings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mextract(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.*\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m(image_.*)\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Split the 'Nom image' column to extract 'imageid' and 'productid'\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m split_columns \u001b[38;5;241m=\u001b[39m \u001b[43mdf_image_train_embeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNom image\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Création des colonnes 'imageid' et 'productid' dans df_image_train_embeddings\u001b[39;00m\n\u001b[0;32m     11\u001b[0m df_image_train_embeddings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimageid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m split_columns[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask_expr\\_str_accessor.py:100\u001b[0m, in \u001b[0;36mStringAccessor.split\u001b[1;34m(self, pat, n, expand)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit\u001b[39m(\u001b[38;5;28mself\u001b[39m, pat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, expand\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     99\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Known inconsistencies: ``expand=True`` with unknown ``n`` will raise a ``NotImplementedError``.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask_expr\\_str_accessor.py:82\u001b[0m, in \u001b[0;36mStringAccessor._split\u001b[1;34m(self, method, pat, n, expand)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expand:\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m     83\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use the expand parameter you must specify the number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     84\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected splits with the n= parameter. Usually n splits \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     85\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult in n+1 output columns.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     86\u001b[0m         )\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_collection(\n\u001b[0;32m     88\u001b[0m         SplitMap(\n\u001b[0;32m     89\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_series,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     94\u001b[0m         )\n\u001b[0;32m     95\u001b[0m     )\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_map(method, pat\u001b[38;5;241m=\u001b[39mpat, n\u001b[38;5;241m=\u001b[39mn, expand\u001b[38;5;241m=\u001b[39mexpand)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: To use the expand parameter you must specify the number of expected splits with the n= parameter. Usually n splits result in n+1 output columns."
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Extraction du nom de l'image à partir du chemin\n",
    "# Correction: s'assurer que le résultat est une série, en extrayant la première colonne de l'opération `extract`\n",
    "df_image_train_embeddings['Nom image'] = df_image_train_embeddings['path'].str.extract(r'.*\\\\(image_.*)')[0]\n",
    "\n",
    "# Split the 'Nom image' column to extract 'imageid' and 'productid'\n",
    "split_columns = df_image_train_embeddings['Nom image'].str.split('_', expand=True)\n",
    "\n",
    "# Création des colonnes 'imageid' et 'productid' dans df_image_train_embeddings\n",
    "df_image_train_embeddings['imageid'] = split_columns[1]\n",
    "df_image_train_embeddings['productid'] = split_columns[3].str.replace('.jpg', '')\n",
    "\n",
    "# Suppression des colonnes 'predictions' et 'Nom image' qui ne sont plus nécessaires\n",
    "# Note: assurez-vous que la colonne 'predictions' existe dans votre DataFrame initial\n",
    "if 'predictions' in df_image_train_embeddings.columns:\n",
    "    df_image_train_embeddings = df_image_train_embeddings.drop(columns=['predictions', 'Nom image'])\n",
    "else:\n",
    "    df_image_train_embeddings = df_image_train_embeddings.drop(columns=['Nom image'])\n",
    "\n",
    "df_image_train_embeddings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_train_embeddings['Nom image'] = df_image_train_embeddings['path'].str.extract(r'.*\\\\(image_.*)')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_columns = df_image_train_embeddings['Nom image'].str.split('_', n=3, expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_train_embeddings['imageid'] = split_columns[1]\n",
    "df_image_train_embeddings['productid'] = split_columns[3].str.replace('.jpg', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_train_embeddings = df_image_train_embeddings.drop(columns=['predictions', 'Nom image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_train_embeddings = df_image_train_embeddings.drop(columns=['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_train_embeddings = df_image_train_embeddings.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
       "       ...\n",
       "       '44992', '44993', '44994', '44995', '44996', '44997', '44998', '44999',\n",
       "       'imageid', 'productid'],\n",
       "      dtype='object', length=45002)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_text_test_embeddings.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
       "       ...\n",
       "       '760', '761', '762', '763', '764', '765', '766', '767', 'imageid',\n",
       "       'productid'],\n",
       "      dtype='object', length=770)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_image_test_embeddings.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
       "       ...\n",
       "       '44992', '44993', '44994', '44995', '44996', '44997', '44998', '44999',\n",
       "       'imageid', 'productid'],\n",
       "      dtype='object', length=45002)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_train_embeddings.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', '0', '1', '2', '3', '4', '5', '6', '7', '8',\n",
       "       ...\n",
       "       '760', '761', '762', '763', '764', '765', '766', '767', 'imageid',\n",
       "       'productid'],\n",
       "      dtype='object', length=771)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_image_train_embeddings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On renomme les colonnes des dataframes\n",
    "df_text_train_embeddings = df_text_train_embeddings.rename(columns=lambda x: f\"{x}_text\" if str(x).isdigit() else x)\n",
    "df_text_test_embeddings = df_text_test_embeddings.rename(columns=lambda x: f\"{x}_text\" if str(x).isdigit() else x)\n",
    "df_image_train_embeddings = df_image_train_embeddings.rename(columns=lambda x: f\"{x}_img\" if str(x).isdigit() else x)\n",
    "df_image_test_embeddings = df_image_test_embeddings.rename(columns=lambda x: f\"{x}_img\" if str(x).isdigit() else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'imageid' and 'productid' to string in both training embeddings dataframes\n",
    "df_text_train_embeddings['imageid'] = df_text_train_embeddings['imageid'].astype(str)\n",
    "df_text_train_embeddings['productid'] = df_text_train_embeddings['productid'].astype(str)\n",
    "\n",
    "df_image_train_embeddings['imageid'] = df_image_train_embeddings['imageid'].astype(str)\n",
    "df_image_train_embeddings['productid'] = df_image_train_embeddings['productid'].astype(str)\n",
    "\n",
    "# Similarly, ensure the test dataframes are also converted if necessary\n",
    "df_text_test_embeddings['imageid'] = df_text_test_embeddings['imageid'].astype(str)\n",
    "df_text_test_embeddings['productid'] = df_text_test_embeddings['productid'].astype(str)\n",
    "\n",
    "df_image_test_embeddings['imageid'] = df_image_test_embeddings['imageid'].astype(str)\n",
    "df_image_test_embeddings['productid'] = df_image_test_embeddings['productid'].astype(str)\n",
    "\n",
    "# Now perform the merge\n",
    "df_train = dd.merge(df_text_train_embeddings, df_image_train_embeddings, on=['imageid', 'productid'], how='inner')\n",
    "df_test = dd.merge(df_text_test_embeddings, df_image_test_embeddings, on=['imageid', 'productid'], how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask_expr.DataFrame'>\n",
      "Columns: 45770 entries, 0_text to 767_img\n",
      "dtypes: object(2), float64(45768)"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask_expr.DataFrame'>\n",
      "Columns: 45770 entries, 0_text to 767_img\n",
      "dtypes: object(2), float64(45768)"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Metadata inference failed in `convert_types`.\n\nYou have supplied a custom function and Dask is unable to \ndetermine the type of output that that function returns. \n\nTo resolve this please provide a meta= keyword.\nThe docstring of the Dask function you ran should have more information.\n\nOriginal error is below:\n------------------------\nValueError(\"invalid literal for int() with base 10: 'foo'\")\n\nTraceback:\n---------\n  File \"c:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask\\dataframe\\utils.py\", line 195, in raise_on_meta_error\n    yield\n  File \"c:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask_expr\\_expr.py\", line 3983, in _emulate\n    return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))\n  File \"C:\\Users\\tgp\\AppData\\Local\\Temp\\ipykernel_15016\\2789118973.py\", line 8, in convert_types\n    df['imageid'] = df['imageid'].astype('int64')\n  File \"c:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py\", line 6532, in astype\n    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\n  File \"c:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 414, in astype\n    return self.apply(\n  File \"c:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 354, in apply\n    applied = getattr(b, f)(**kwargs)\n  File \"c:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 616, in astype\n    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\n  File \"c:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 238, in astype_array_safe\n    new_values = astype_array(values, dtype, copy=copy)\n  File \"c:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 183, in astype_array\n    values = _astype_nansafe(values, dtype, copy=copy)\n  File \"c:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 134, in _astype_nansafe\n    return arr.astype(dtype, copy=True)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask\\dataframe\\utils.py:195\u001b[0m, in \u001b[0;36mraise_on_meta_error\u001b[1;34m(funcname, udf)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 195\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask_expr\\_expr.py:3983\u001b[0m, in \u001b[0;36m_emulate\u001b[1;34m(func, udf, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3982\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m raise_on_meta_error(funcname(func), udf\u001b[38;5;241m=\u001b[39mudf):\n\u001b[1;32m-> 3983\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m_extract_meta(args, \u001b[38;5;28;01mTrue\u001b[39;00m), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_extract_meta(kwargs, \u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36mconvert_types\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimageid\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m----> 8\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimageid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimageid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mint64\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproductid\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:6532\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   6530\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6531\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[1;32m-> 6532\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6533\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:414\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    412\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:354\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 354\u001b[0m     applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    355\u001b[0m result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:616\u001b[0m, in \u001b[0;36mBlock.astype\u001b[1;34m(self, dtype, copy, errors, using_cow)\u001b[0m\n\u001b[0;32m    614\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m--> 616\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    618\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:238\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[1;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 238\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:183\u001b[0m, in \u001b[0;36mastype_array\u001b[1;34m(values, dtype, copy)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 183\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:134\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'foo'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# conversion de types à df_train et df_test\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m df_train \u001b[38;5;241m=\u001b[39m \u001b[43mdf_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_partitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_types\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m df_test \u001b[38;5;241m=\u001b[39m df_test\u001b[38;5;241m.\u001b[39mmap_partitions(convert_types)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_train\u001b[38;5;241m.\u001b[39mdtypes)\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask_expr\\_collection.py:1090\u001b[0m, in \u001b[0;36mFrameBase.map_partitions\u001b[1;34m(self, func, meta, enforce_metadata, transform_divisions, clear_divisions, align_dataframes, parent_meta, *args, **kwargs)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[38;5;129m@insert_meta_param_description\u001b[39m(pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_partitions\u001b[39m(\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    977\u001b[0m ):\n\u001b[0;32m    978\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply a Python function to each partition\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \n\u001b[0;32m    980\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;124;03m    None as the division.\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1090\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m map_partitions(\n\u001b[0;32m   1091\u001b[0m         func,\n\u001b[0;32m   1092\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1093\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m   1094\u001b[0m         meta\u001b[38;5;241m=\u001b[39mmeta,\n\u001b[0;32m   1095\u001b[0m         enforce_metadata\u001b[38;5;241m=\u001b[39menforce_metadata,\n\u001b[0;32m   1096\u001b[0m         transform_divisions\u001b[38;5;241m=\u001b[39mtransform_divisions,\n\u001b[0;32m   1097\u001b[0m         clear_divisions\u001b[38;5;241m=\u001b[39mclear_divisions,\n\u001b[0;32m   1098\u001b[0m         align_dataframes\u001b[38;5;241m=\u001b[39malign_dataframes,\n\u001b[0;32m   1099\u001b[0m         parent_meta\u001b[38;5;241m=\u001b[39mparent_meta,\n\u001b[0;32m   1100\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1101\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask_expr\\_collection.py:6108\u001b[0m, in \u001b[0;36mmap_partitions\u001b[1;34m(func, meta, enforce_metadata, transform_divisions, clear_divisions, align_dataframes, parent_meta, *args, **kwargs)\u001b[0m\n\u001b[0;32m   6094\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n\u001b[0;32m   6095\u001b[0m new_expr \u001b[38;5;241m=\u001b[39m expr\u001b[38;5;241m.\u001b[39mMapPartitions(\n\u001b[0;32m   6096\u001b[0m     args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   6097\u001b[0m     func,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6106\u001b[0m     \u001b[38;5;241m*\u001b[39margs[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m   6107\u001b[0m )\n\u001b[1;32m-> 6108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_expr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask_expr\\_collection.py:4764\u001b[0m, in \u001b[0;36mnew_collection\u001b[1;34m(expr)\u001b[0m\n\u001b[0;32m   4762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_collection\u001b[39m(expr):\n\u001b[0;32m   4763\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create new collection from an expr\"\"\"\u001b[39;00m\n\u001b[1;32m-> 4764\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[43mexpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_meta\u001b[49m\n\u001b[0;32m   4765\u001b[0m     expr\u001b[38;5;241m.\u001b[39m_name  \u001b[38;5;66;03m# Ensure backend is imported\u001b[39;00m\n\u001b[0;32m   4766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_collection_type(meta)(expr)\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\functools.py:981\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m    979\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[1;32m--> 981\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    983\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask_expr\\_expr.py:630\u001b[0m, in \u001b[0;36mMapPartitions._meta\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcached_property\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_meta\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    629\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperand(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_meta_map_partitions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mExpr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent_meta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask_expr\\_expr.py:3997\u001b[0m, in \u001b[0;36m_get_meta_map_partitions\u001b[1;34m(args, dfs, func, kwargs, meta, parent_meta)\u001b[0m\n\u001b[0;32m   3993\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m meta \u001b[38;5;129;01mis\u001b[39;00m no_default:\n\u001b[0;32m   3994\u001b[0m     \u001b[38;5;66;03m# Use non-normalized kwargs here, as we want the real values (not\u001b[39;00m\n\u001b[0;32m   3995\u001b[0m     \u001b[38;5;66;03m# delayed values)\u001b[39;00m\n\u001b[0;32m   3996\u001b[0m     a \u001b[38;5;241m=\u001b[39m [meta_nonempty(arg\u001b[38;5;241m.\u001b[39m_meta) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, Expr) \u001b[38;5;28;01melse\u001b[39;00m arg \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m-> 3997\u001b[0m     meta \u001b[38;5;241m=\u001b[39m _emulate(func, \u001b[38;5;241m*\u001b[39ma, udf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3998\u001b[0m     meta_is_emulated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3999\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask_expr\\_expr.py:3982\u001b[0m, in \u001b[0;36m_emulate\u001b[1;34m(func, udf, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3977\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_emulate\u001b[39m(func, \u001b[38;5;241m*\u001b[39margs, udf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   3978\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3979\u001b[0m \u001b[38;5;124;03m    Apply a function using args / kwargs. If arguments contain dd.DataFrame /\u001b[39;00m\n\u001b[0;32m   3980\u001b[0m \u001b[38;5;124;03m    dd.Series, using internal cache (``_meta``) for calculation\u001b[39;00m\n\u001b[0;32m   3981\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3982\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m raise_on_meta_error(funcname(func), udf\u001b[38;5;241m=\u001b[39mudf):\n\u001b[0;32m   3983\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m_extract_meta(args, \u001b[38;5;28;01mTrue\u001b[39;00m), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_extract_meta(kwargs, \u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    151\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask\\dataframe\\utils.py:216\u001b[0m, in \u001b[0;36mraise_on_meta_error\u001b[1;34m(funcname, udf)\u001b[0m\n\u001b[0;32m    207\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal error is below:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    214\u001b[0m )\n\u001b[0;32m    215\u001b[0m msg \u001b[38;5;241m=\u001b[39m msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m funcname \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mrepr\u001b[39m(e), tb)\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Metadata inference failed in `convert_types`.\n\nYou have supplied a custom function and Dask is unable to \ndetermine the type of output that that function returns. \n\nTo resolve this please provide a meta= keyword.\nThe docstring of the Dask function you ran should have more information.\n\nOriginal error is below:\n------------------------\nValueError(\"invalid literal for int() with base 10: 'foo'\")\n\nTraceback:\n---------\n  File \"c:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask\\dataframe\\utils.py\", line 195, in raise_on_meta_error\n    yield\n  File \"c:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask_expr\\_expr.py\", line 3983, in _emulate\n    return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))\n  File \"C:\\Users\\tgp\\AppData\\Local\\Temp\\ipykernel_15016\\2789118973.py\", line 8, in convert_types\n    df['imageid'] = df['imageid'].astype('int64')\n  File \"c:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py\", line 6532, in astype\n    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\n  File \"c:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 414, in astype\n    return self.apply(\n  File \"c:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 354, in apply\n    applied = getattr(b, f)(**kwargs)\n  File \"c:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 616, in astype\n    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\n  File \"c:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 238, in astype_array_safe\n    new_values = astype_array(values, dtype, copy=copy)\n  File \"c:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 183, in astype_array\n    values = _astype_nansafe(values, dtype, copy=copy)\n  File \"c:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py\", line 134, in _astype_nansafe\n    return arr.astype(dtype, copy=True)\n"
     ]
    }
   ],
   "source": [
    "def convert_types(df):\n",
    "    # Convertion de toutes les colonnes float en float32\n",
    "    float_cols = df.select_dtypes(include=['float64']).columns\n",
    "    df[float_cols] = df[float_cols].astype('float32')\n",
    "    \n",
    "    # 'imageid' et 'productid' de type string\n",
    "    if 'imageid' in df.columns:\n",
    "        df['imageid'] = df['imageid'].astype('int64')\n",
    "    if 'productid' in df.columns:\n",
    "        df['productid'] = df['productid'].astype('int64')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# conversion de types à df_train et df_test\n",
    "df_train = df_train.map_partitions(convert_types)\n",
    "df_test = df_test.map_partitions(convert_types)\n",
    "\n",
    "print(df_train.dtypes)\n",
    "print(df_test.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['imageid'] = df_train['imageid'].astype('int64')\n",
    "df_train['productid'] = df_train['productid'].astype('int64')\n",
    "\n",
    "df_test['imageid'] = df_test['imageid'].astype('int64')\n",
    "df_test['productid'] = df_test['productid'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask_expr.DataFrame'>\n",
      "Columns: 3 entries, imageid to prdtypecode\n",
      "dtypes: int64(3)"
     ]
    }
   ],
   "source": [
    "df_classes = dd.from_pandas(df_classes, npartitions=10)\n",
    "df_classes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classes['productid'] = df_classes['productid'].astype('int64')\n",
    "df_classes['imageid'] = df_classes['imageid'].astype('int64')\n",
    "df_classes['imageid'] = df_classes['prdtypecode'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout de la colonne prdtypecode\n",
    "df_train = dd.merge(df_train, df_classes[['productid', 'prdtypecode']], on='productid', how='left')\n",
    "df_test = dd.merge(df_test, df_classes[['productid', 'prdtypecode']], on='productid', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['0_text', '1_text', '2_text', '3_text', '4_text', '5_text', '6_text',\n",
       "       '7_text', '8_text', '9_text',\n",
       "       ...\n",
       "       '759_img', '760_img', '761_img', '762_img', '763_img', '764_img',\n",
       "       '765_img', '766_img', '767_img', 'prdtypecode'],\n",
       "      dtype='object', length=45771)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['0_text', '1_text', '2_text', '3_text', '4_text', '5_text', '6_text',\n",
       "       '7_text', '8_text', '9_text',\n",
       "       ...\n",
       "       '759_img', '760_img', '761_img', '762_img', '763_img', '764_img',\n",
       "       '765_img', '766_img', '767_img', 'prdtypecode'],\n",
       "      dtype='object', length=45771)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask_expr.DataFrame'>\n",
      "Columns: 45771 entries, 0_text to prdtypecode\n",
      "dtypes: float64(45768), int64(3)"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask_expr.DataFrame'>\n",
      "Columns: 45771 entries, 0_text to prdtypecode\n",
      "dtypes: float64(45768), int64(3)"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TokenizationError",
     "evalue": "Object <function write_partition_to_sql at 0x0000027B69933640> cannot be deterministically hashed. See https://docs.dask.org/en/latest/custom-collections.html#implementing-deterministic-hashing for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask\\base.py:1212\u001b[0m, in \u001b[0;36mnormalize_object\u001b[1;34m(o)\u001b[0m\n\u001b[0;32m   1211\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_normalize_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask\\base.py:1248\u001b[0m, in \u001b[0;36m_normalize_pickle\u001b[1;34m(o)\u001b[0m\n\u001b[0;32m   1247\u001b[0m     buffers\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m-> 1248\u001b[0m     pik \u001b[38;5;241m=\u001b[39m \u001b[43mcloudpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hash_buffer_hex(pik), [hash_buffer_hex(buf) \u001b[38;5;28;01mfor\u001b[39;00m buf \u001b[38;5;129;01min\u001b[39;00m buffers]\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\cloudpickle\\cloudpickle.py:1479\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[0;32m   1478\u001b[0m cp \u001b[38;5;241m=\u001b[39m Pickler(file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback)\n\u001b[1;32m-> 1479\u001b[0m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\cloudpickle\\cloudpickle.py:1245\u001b[0m, in \u001b[0;36mPickler.dump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot pickle 'sqlalchemy.cprocessors.UnicodeResultProcessor' object",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTokenizationError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Appliquez la fonction à chaque partition de df_train\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProgressBar():\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mdf_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_partitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite_partition_to_sql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtable_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcompute(scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Appliquez la fonction à chaque partition de df_test\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProgressBar():\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask_expr\\_collection.py:1090\u001b[0m, in \u001b[0;36mFrameBase.map_partitions\u001b[1;34m(self, func, meta, enforce_metadata, transform_divisions, clear_divisions, align_dataframes, parent_meta, *args, **kwargs)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[38;5;129m@insert_meta_param_description\u001b[39m(pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_partitions\u001b[39m(\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    977\u001b[0m ):\n\u001b[0;32m    978\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply a Python function to each partition\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \n\u001b[0;32m    980\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;124;03m    None as the division.\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1090\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m map_partitions(\n\u001b[0;32m   1091\u001b[0m         func,\n\u001b[0;32m   1092\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1093\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m   1094\u001b[0m         meta\u001b[38;5;241m=\u001b[39mmeta,\n\u001b[0;32m   1095\u001b[0m         enforce_metadata\u001b[38;5;241m=\u001b[39menforce_metadata,\n\u001b[0;32m   1096\u001b[0m         transform_divisions\u001b[38;5;241m=\u001b[39mtransform_divisions,\n\u001b[0;32m   1097\u001b[0m         clear_divisions\u001b[38;5;241m=\u001b[39mclear_divisions,\n\u001b[0;32m   1098\u001b[0m         align_dataframes\u001b[38;5;241m=\u001b[39malign_dataframes,\n\u001b[0;32m   1099\u001b[0m         parent_meta\u001b[38;5;241m=\u001b[39mparent_meta,\n\u001b[0;32m   1100\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1101\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask_expr\\_collection.py:6095\u001b[0m, in \u001b[0;36mmap_partitions\u001b[1;34m(func, meta, enforce_metadata, transform_divisions, clear_divisions, align_dataframes, parent_meta, *args, **kwargs)\u001b[0m\n\u001b[0;32m   6087\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m align_dataframes:\n\u001b[0;32m   6088\u001b[0m     \u001b[38;5;66;03m# TODO: Handle alignment?\u001b[39;00m\n\u001b[0;32m   6089\u001b[0m     \u001b[38;5;66;03m# Perhaps we only handle the case that all `Expr` operands\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6092\u001b[0m     \u001b[38;5;66;03m# will need to call `Repartition` on operands that are not\u001b[39;00m\n\u001b[0;32m   6093\u001b[0m     \u001b[38;5;66;03m# aligned with `self.expr`.\u001b[39;00m\n\u001b[0;32m   6094\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n\u001b[1;32m-> 6095\u001b[0m new_expr \u001b[38;5;241m=\u001b[39m \u001b[43mexpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMapPartitions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   6096\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6099\u001b[0m \u001b[43m    \u001b[49m\u001b[43menforce_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_divisions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclear_divisions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6102\u001b[0m \u001b[43m    \u001b[49m\u001b[43malign_dataframes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent_meta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6106\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6107\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_collection(new_expr)\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask_expr\\_core.py:57\u001b[0m, in \u001b[0;36mExpr.__new__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m inst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m     56\u001b[0m inst\u001b[38;5;241m.\u001b[39moperands \u001b[38;5;241m=\u001b[39m [_unpack_collections(o) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m operands]\n\u001b[1;32m---> 57\u001b[0m _name \u001b[38;5;241m=\u001b[39m \u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _name \u001b[38;5;129;01min\u001b[39;00m Expr\u001b[38;5;241m.\u001b[39m_instances:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Expr\u001b[38;5;241m.\u001b[39m_instances[_name]\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\functools.py:981\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m    979\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[1;32m--> 981\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    983\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask_expr\\_expr.py:617\u001b[0m, in \u001b[0;36mMapPartitions._name\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    616\u001b[0m     head \u001b[38;5;241m=\u001b[39m funcname(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m--> 617\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m head \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43m_tokenize_deterministic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask_expr\\_util.py:103\u001b[0m, in \u001b[0;36m_tokenize_deterministic\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_tokenize_deterministic\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;66;03m# Utility to be strict about deterministic tokens\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config\u001b[38;5;241m.\u001b[39mset({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenize.ensure-deterministic\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}):\n\u001b[1;32m--> 103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tokenize(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask\\base.py:1033\u001b[0m, in \u001b[0;36mtokenize\u001b[1;34m(ensure_deterministic, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Deterministic token\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m \n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m>>> tokenize([1, 2, '3'])\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1030\u001b[0m \u001b[38;5;124;03m    Defaults to the `tokenize.ensure-deterministic` configuration parameter.\u001b[39;00m\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _seen_ctx(reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), _ensure_deterministic_ctx(ensure_deterministic):\n\u001b[1;32m-> 1033\u001b[0m     token: \u001b[38;5;28mobject\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43m_normalize_seq_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1034\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m   1035\u001b[0m         token \u001b[38;5;241m=\u001b[39m token, _normalize_seq_func(\u001b[38;5;28msorted\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mitems()))\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask\\base.py:1159\u001b[0m, in \u001b[0;36m_normalize_seq_func\u001b[1;34m(seq)\u001b[0m\n\u001b[0;32m   1157\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1158\u001b[0m         seen[\u001b[38;5;28mid\u001b[39m(item)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(seen), item\n\u001b[1;32m-> 1159\u001b[0m         item \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1160\u001b[0m     out\u001b[38;5;241m.\u001b[39mappend(item)\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask\\utils.py:773\u001b[0m, in \u001b[0;36mDispatch.__call__\u001b[1;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    770\u001b[0m \u001b[38;5;124;03mCall the corresponding method based on type of argument.\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    772\u001b[0m meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\u001b[38;5;28mtype\u001b[39m(arg))\n\u001b[1;32m--> 773\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m meth(arg, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask\\base.py:1214\u001b[0m, in \u001b[0;36mnormalize_object\u001b[1;34m(o)\u001b[0m\n\u001b[0;32m   1212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _normalize_pickle(o)\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m-> 1214\u001b[0m     \u001b[43m_maybe_raise_nondeterministic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1215\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mObject \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mo\u001b[49m\u001b[38;5;132;43;01m!r}\u001b[39;49;00m\u001b[38;5;124;43m cannot be deterministically hashed. See \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m   1216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://docs.dask.org/en/latest/custom-collections.html#implementing-deterministic-hashing \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m   1217\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfor more information.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m uuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;241m.\u001b[39mhex\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask\\base.py:1078\u001b[0m, in \u001b[0;36m_maybe_raise_nondeterministic\u001b[1;34m(msg)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _ensure_deterministic_ctx(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m ensure_deterministic:\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ensure_deterministic:\n\u001b[1;32m-> 1078\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TokenizationError(msg)\n",
      "\u001b[1;31mTokenizationError\u001b[0m: Object <function write_partition_to_sql at 0x0000027B69933640> cannot be deterministically hashed. See https://docs.dask.org/en/latest/custom-collections.html#implementing-deterministic-hashing for more information."
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# Configuration de la connexion à la base de données\n",
    "engine = create_engine('postgresql://postgres:*****@localhost/mydb')\n",
    "\n",
    "def write_partition_to_sql(df, name):\n",
    "    # Convertit la partition Dask en Pandas dataframe\n",
    "    df = df.compute()\n",
    "    # Écrit la partition dans la base de données\n",
    "    df.to_sql(name, engine, if_exists='append', index=False)\n",
    "\n",
    "# Appliquez la fonction à chaque partition de df_train\n",
    "with ProgressBar():\n",
    "    df_train.map_partitions(write_partition_to_sql, 'table_train').compute(scheduler='threads')\n",
    "\n",
    "# Appliquez la fonction à chaque partition de df_test\n",
    "with ProgressBar():\n",
    "    df_test.map_partitions(write_partition_to_sql, 'table_test').compute(scheduler='threads')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#####                                   ] | 14% Completed | 11hr 13m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Apply the function to each partition\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProgressBar():\n\u001b[1;32m---> 22\u001b[0m     \u001b[43mdf_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_partitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite_partition_to_sql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtable_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProgressBar():\n\u001b[0;32m     25\u001b[0m     df_test\u001b[38;5;241m.\u001b[39mmap_partitions(write_partition_to_sql, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable_test\u001b[39m\u001b[38;5;124m'\u001b[39m, meta\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame(meta))\u001b[38;5;241m.\u001b[39mcompute(scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask_expr\\_collection.py:476\u001b[0m, in \u001b[0;36mFrameBase.compute\u001b[1;34m(self, fuse, **kwargs)\u001b[0m\n\u001b[0;32m    474\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mrepartition(npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    475\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39moptimize(fuse\u001b[38;5;241m=\u001b[39mfuse)\n\u001b[1;32m--> 476\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DaskMethodsMixin\u001b[38;5;241m.\u001b[39mcompute(out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask\\base.py:375\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    352\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 375\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask\\base.py:661\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    658\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 661\u001b[0m     results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from dask.diagnostics import ProgressBar\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "\n",
    "def write_partition_to_sql(df, name):\n",
    "    if isinstance(df, dd.DataFrame):\n",
    "        df = df.compute()  # This converts Dask DataFrame to Pandas DataFrame\n",
    "    \n",
    "    # Create engine inside the function\n",
    "    engine = create_engine('postgresql://postgres:*****@localhost/mydb')\n",
    "    \n",
    "    # Write the DataFrame to SQL\n",
    "    df.to_sql(name, engine, if_exists='append', index=False)\n",
    "\n",
    "# Meta data definition, assuming the structure of your DataFrame\n",
    "meta = {'column1': pd.Series([], dtype='int'),\n",
    "        'column2': pd.Series([], dtype='str')}\n",
    "\n",
    "# Apply the function to each partition\n",
    "with ProgressBar():\n",
    "    df_train.map_partitions(write_partition_to_sql, 'table_train', meta=pd.DataFrame(meta)).compute(scheduler='threads')\n",
    "\n",
    "with ProgressBar():\n",
    "    df_test.map_partitions(write_partition_to_sql, 'table_test', meta=pd.DataFrame(meta)).compute(scheduler='threads')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sqlalchemy' has no attribute 'dialects'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [77]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Integer, String, Float, Boolean\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Configuration de la connexion à la base de données\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpostgresql://postgres:1motdepasse@localhost/mydb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfer_sqlalchemy_types\u001b[39m(df):\n\u001b[0;32m      9\u001b[0m     dmap \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m: Integer,\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m: Float,\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbool\u001b[39m\u001b[38;5;124m'\u001b[39m: Boolean,\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m: String,\n\u001b[0;32m     14\u001b[0m     }\n",
      "File \u001b[1;32m<string>:2\u001b[0m, in \u001b[0;36mcreate_engine\u001b[1;34m(url, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sqlalchemy\\util\\deprecations.py:375\u001b[0m, in \u001b[0;36mdeprecated_params.<locals>.decorate.<locals>.warned\u001b[1;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m    369\u001b[0m         _warn_with_version(\n\u001b[0;32m    370\u001b[0m             messages[m],\n\u001b[0;32m    371\u001b[0m             versions[m],\n\u001b[0;32m    372\u001b[0m             version_warnings[m],\n\u001b[0;32m    373\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m    374\u001b[0m         )\n\u001b[1;32m--> 375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sqlalchemy\\engine\\create.py:522\u001b[0m, in \u001b[0;36mcreate_engine\u001b[1;34m(url, **kwargs)\u001b[0m\n\u001b[0;32m    518\u001b[0m u \u001b[38;5;241m=\u001b[39m _url\u001b[38;5;241m.\u001b[39mmake_url(url)\n\u001b[0;32m    520\u001b[0m u, plugins, kwargs \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39m_instantiate_plugins(kwargs)\n\u001b[1;32m--> 522\u001b[0m entrypoint \u001b[38;5;241m=\u001b[39m \u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_entrypoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    523\u001b[0m dialect_cls \u001b[38;5;241m=\u001b[39m entrypoint\u001b[38;5;241m.\u001b[39mget_dialect_cls(u)\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_coerce_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sqlalchemy\\engine\\url.py:662\u001b[0m, in \u001b[0;36mURL._get_entrypoint\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    661\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrivername\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 662\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;66;03m# check for legacy dialects that\u001b[39;00m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;66;03m# would return a module with 'dialect' as the\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;66;03m# actual class\u001b[39;00m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    667\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdialect\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdialect, \u001b[38;5;28mtype\u001b[39m)\n\u001b[0;32m    669\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdialect, Dialect)\n\u001b[0;32m    670\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:333\u001b[0m, in \u001b[0;36mPluginLoader.load\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimpls[name]()\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_fn:\n\u001b[1;32m--> 333\u001b[0m     loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loader:\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimpls[name] \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[1;32mc:\\Users\\tgp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sqlalchemy\\dialects\\__init__.py:58\u001b[0m, in \u001b[0;36m_auto_fn\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39mloader(driver)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msqlalchemy.dialects.\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialects\u001b[49m\n\u001b[0;32m     59\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, dialect)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'sqlalchemy' has no attribute 'dialects'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.types import Integer, String, Float, Boolean\n",
    "\n",
    "# Configuration de la connexion à la base de données\n",
    "engine = create_engine('postgresql://postgres:*****@localhost/mydb')\n",
    "\n",
    "def infer_sqlalchemy_types(df):\n",
    "    dmap = {\n",
    "        'int64': Integer,\n",
    "        'float64': Float,\n",
    "        'bool': Boolean,\n",
    "        'object': String,\n",
    "    }\n",
    "    return {col: dmap[str(df[col].dtype)] for col in df.columns}\n",
    "\n",
    "def create_table_and_load_data(filename, table_name):\n",
    "    # Lire le fichier CSV avec pandas\n",
    "    df = pd.read_csv(filename, nrows=0)  # Charger seulement l'en-tête pour inférer les types\n",
    "\n",
    "    # Inférer les types de colonnes pour SQLAlchemy\n",
    "    types = infer_sqlalchemy_types(df)\n",
    "\n",
    "    # Lire et charger le fichier par chunks dans PostgreSQL\n",
    "    chunksize = 500  # Adapter selon la mémoire disponible\n",
    "    for chunk in pd.read_csv(filename, chunksize=chunksize, dtype=str):\n",
    "        chunk.to_sql(table_name, engine, if_exists='append', index=False, dtype=types)\n",
    "\n",
    "# Exemple d'utilisation\n",
    "create_table_and_load_data('C:/Users/tgp/Documents/kaggle/Rakuten/Github_final/4_Prediction_catégories/final_image_test_embeddings_clean.csv', 'final_image_test_embeddings_clean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
